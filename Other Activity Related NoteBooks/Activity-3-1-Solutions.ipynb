{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbors using LSH\n",
    "\n",
    "This notebook will show the give and take between building multiple hash tables and employing multiple hash functions when searching for approximate neighbors using LSH. The demonstration is meant to clarify slide 12 from slide deck 3, on Nearest Neighbors.\n",
    "\n",
    "I have written a basic LSH implementation in Python, with instantiations for the cosine similarity, Hamming distance, and the Jaccard coefficient LSH families. The code is written in OOP style and can be easily extended to other LSH families. Take a look at lsh.py for the details. In this demo, we will be using the cosine similarity version of the LSH data structure, *clsh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import numpy as np\n",
    "from lsh import clsh, jlsh, generateSamples, findNeighborsBrute, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate some random samples, and split the data into train (X) and test (Y) subsets. Samples are generated from 100 gausian blobs, i.e., points will be fairly spread out as far as their cosine similarity is concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generateSamples(nsamples=1000, nfeatures=100, nclusters=64, clusterstd=50, binary=False)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic concept in LSH is that of *hashing* the vectors using a random LSH family of hash functions. As we discussed in class, the LSH families will be more likely to assign the same hash value to similar items. This, however, does not happen all the time. First, let's see what the result of hashing a vector looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L11 = clsh(X, ntables=1, nfunctions=1)\n",
    "for i in range(10):\n",
    "    print(L11.hash(X[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in slide 18, the output of the cosine family of LSH function is binary, depending on the sign of the dot-product $\\langle r,x\\rangle$ between the random unit vector $r$ and our input vector $x=X[i,:]$.\n",
    "\n",
    "Note that we created a single table in our LSH data structure and are using a single LSH function to hash vectors. This means that we're simply partitioning vectors into two buckets. Some vectors will go to the bucket with ID 0, and others will go to the bucket with ID 1.\n",
    "\n",
    "When we instantiated the LSH data structure *L*, all the vectors in X were already assigned to their respective buckets. Let's see how many vectors each bucket has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Bucket ID 0 has %d vectors.\" % len(L11.tables[0]['0']))\n",
    "print(\"Bucket ID 1 has %d vectors.\" % len(L11.tables[0]['1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when it's time to find neighbors for a new vector, say $y=Y[0,:]$, the first vector in our test set, we hash the vector to see which bucket we should look in to find neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(L11.hash(Y[0,:], tid=0, fid=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I passed in the ID of the table I'm searching in and the ID of the function I'm hashing with. For LSH to work, we have to use the same hashing functions that were used to create the table(s). Therefore, $clsh$ stores the randomly generated functions it created for each table.\n",
    "\n",
    "Now, it looks like I have to compare $y$ against almost half of the vectors in $X$, which is a lot, and leads to low *precision*. Precision is the fraction of retrieved instances (the vectors we compared against) that are relevant (that would also be in the exact result). Since the number of objects we're comparing against is high, precision will be low. In order to increase the precision, I can use several hash functions and concatenate their results. Increasing the precision will also reduc the amount of time spent finding neighbors, as we will have fewer objects to compare against.\n",
    "\n",
    "Let's say I use 2 hash functions from the Cosine LSH family. Then, the possible resulting hash values would be 00, 01, 10, and 11, spliting the vectors in X into 4 buckets (instead of 2, when we used 1 function). If we use 3 functions, we get 8 buckets. In general, using $f$ functions will split the \"search space\" into $2^f$ buckets.\n",
    "\n",
    "Let's try this using 3 functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L13 = clsh(X, ntables=1, nfunctions=3)\n",
    "for k in L13.tables[0].keys():\n",
    "    print(\"Bucket ID %s has %d vectors.\" % (k, len(L13.tables[0][k])))\n",
    "    \n",
    "print(\"\\nWe only need to compare y against vectors in bucket %s.\" % L13.signature(Y[0,:], tid=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: Note that in this academic LSH implementation we use a simple way to generate bucket IDs. We concatenate the string representation of the resulting hash value from each hash function. LSH libraries often implement a secondary (exact) hash function for generating numeric IDs for the buckets. A similar scheme is proposed in the LSH reference I nored on Canvas: [SPM'08] Malcolm Slaney and Michael Casey. Locality-Sensitive Hashing for Finding Nearest Neighbors. Lecture Notes. IEEE Signal Processing Magazine, 2008.\n",
    "\n",
    "It is easy to see we now have much fewer vectors to compare against when we search for $y$'s neighbors. However, some of the true neighbors may have been accidentally placed in other buckets, which lowers *recall*. Recall (also known in Statistics references as *sensitivity*) is the fraction of relevant instances that are retrieved, i.e., the fraction of true neighbors in our top-$k$ divided by $k$. \n",
    "\n",
    "Let's compare the mean recall for finding neighbors using 1 hash function vs. 3 hash functions. To do that, we will first have to find the \"true neighbors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 100  # number of neighbors to find\n",
    "nbrsExact = findNeighborsBrute(X, Y, k=k, sim=\"cos\")\n",
    "print(\"Number of computed similarities for the brute-force approach: %d.\" % (X.shape[0] * Y.shape[0]))\n",
    "nbrsTest11  = L11.findNeighbors(Y, k=k)\n",
    "nbrsTest13  = L13.findNeighbors(Y, k=k)\n",
    "print(\"Recall with 1 hash function: %f. Number of computed similarities: %d.\" % (recall(nbrsTest11, nbrsExact), L11.nsims))\n",
    "print(\"Recall with 3 hash functions: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the recall by building several LSH tables instead of one. Then, instead of looking in one bucket for $y$'s neighbors, we will be looking in one bucket in each table. The search method gets the set union of object IDs in all these buckets, and then computes similarities against all of them.\n",
    "\n",
    "### Excercise 1\n",
    "\n",
    "Compare the mean recall for finding neighbors using 1 table vs. 3 tables, when each table uses 3 hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L33 = clsh(X, ntables=3, nfunctions=3)\n",
    "nbrsTest33  = L33.findNeighbors(Y, k=k)\n",
    "print(\"Recall with 1 table: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))\n",
    "print(\"Recall with 3 tables: %f. Number of computed similarities: %d.\" % (recall(nbrsTest33, nbrsExact), L33.nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given high enough # tables and # hashes (hash functions), we can achieve high recall and precision, sometimes at the expense of efficiency.\n",
    "\n",
    "### Excercise 2\n",
    "\n",
    "Find the minimum number of tables necessary to obtain `recall` of at least `0.90` using 3 functions. What is the number of computed similarities for that LSH forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Testing # tables: \",\n",
    "for i in xrange(1,31):\n",
    "    print i,\n",
    "    L = clsh(X, ntables=i, nfunctions=3)\n",
    "    nbrsTest  = L.findNeighbors(Y, k=100)\n",
    "    if recall(nbrsTest, nbrsExact) >= 0.9:\n",
    "        print(\"\\nNumber of tables: %d. Recall: %f. Number of computed similarities: %d.\" % \n",
    "              (i, recall(nbrsTest, nbrsExact), L.nsims))\n",
    "        break\n",
    "if recall(nbrsTest, nbrsExact) < 0.9:\n",
    "    print(\"\\nNumber of tables: %d. Recall: %f. Number of computed similarities: %d.\" % \n",
    "          (i, recall(nbrsTest, nbrsExact), L.nsims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Repeat Exrcise 1 and 2 using `Jaccard Coefficient` instead of `cosine similarity`. Note that you will need to re-generate samples using the `binary=True` parameter and re-compute `nbrsExact` for the new similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xb, Yb = generateSamples(nsamples=1000, nfeatures=100, nclusters=64, clusterstd=50, binary=True)\n",
    "nbrsExact = findNeighborsBrute(Xb, Yb, k=k, sim=\"jac\")\n",
    "print(\"Number of computed similarities for the brute-force approach: %d.\" % (Xb.shape[0] * Yb.shape[0]))\n",
    "L11 = jlsh(Xb, ntables=1, nfunctions=1)\n",
    "L13 = jlsh(Xb, ntables=1, nfunctions=3)\n",
    "nbrsTest11  = L11.findNeighbors(Yb, k=k)\n",
    "nbrsTest13  = L13.findNeighbors(Yb, k=k)\n",
    "print(\"Recall with 1 hash function: %f. Number of computed similarities: %d.\" % (recall(nbrsTest11, nbrsExact), L11.nsims))\n",
    "print(\"Recall with 3 hash functions: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))\n",
    "L33 = jlsh(Xb, ntables=3, nfunctions=3)\n",
    "nbrsTest33  = L33.findNeighbors(Yb, k=k)\n",
    "print(\"Recall with 1 table: %f. Number of computed similarities: %d.\" % (recall(nbrsTest13, nbrsExact), L13.nsims))\n",
    "print(\"Recall with 3 tables: %f. Number of computed similarities: %d.\" % (recall(nbrsTest33, nbrsExact), L33.nsims))\n",
    "print \"Testing # tables: \",\n",
    "for i in xrange(1,51):\n",
    "    print i,\n",
    "    L = jlsh(Xb, ntables=i, nfunctions=3)\n",
    "    nbrsTest  = L.findNeighbors(Yb, k=100)\n",
    "    if recall(nbrsTest, nbrsExact) >= 0.9:\n",
    "        print(\"\\nNumber of tables: %d. Recall: %f. Number of computed similarities: %d.\" % \n",
    "              (i, recall(nbrsTest, nbrsExact), L.nsims))\n",
    "        break\n",
    "if recall(nbrsTest, nbrsExact) < 0.9:\n",
    "    print(\"\\nNumber of tables: %d. Recall: %f. Number of computed similarities: %d.\" % \n",
    "          (i, recall(nbrsTest, nbrsExact), L.nsims))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
